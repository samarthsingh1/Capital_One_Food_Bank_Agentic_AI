
â¸»

ğŸ§  AI-Powered Jira Ticket Analytics Pipeline

This project is an end-to-end real-time analytics pipeline for processing, analyzing, and visualizing Jira-style consumer complaint tickets (e.g., food bank issues). It includes:
	â€¢	FastAPI for RESTful ingestion
	â€¢	Kafka for message queuing
	â€¢	Avro + Schema Registry for serialization
	â€¢	ClickHouse for ultra-fast OLAP-style storage
	â€¢	Streamlit for rich, multi-page dashboards
	â€¢	Agentic AI (via Gemini or Ollama) for summarization and urgency/prioritization

â¸»

ğŸ“ Project Structure

.
â”œâ”€â”€ Ingest/ <br>
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ services/              # Kafka producer & ClickHouse consumer
â”‚   â”‚   â”œâ”€â”€ routes/                # FastAPI routes (ticket ingestion)
â”‚   â”‚   â”œâ”€â”€ schemas/               # Avro schema file
â”‚   â”‚   â””â”€â”€ main.py                # FastAPI app entry point
â”‚   â”œâ”€â”€ docker-compose.yml        # Kafka & Zookeeper Docker setup
â”‚   â””â”€â”€ ticket.avsc               # Avro schema for Kafka serialization
â”œâ”€â”€ Modules/
â”‚   â”œâ”€â”€ db.py                     # ClickHouse query connector
â”‚   â”œâ”€â”€ preprocessing.py          # Data cleaning
â”‚   â”œâ”€â”€ forecasting.py            # Time series logic
â”‚   â”œâ”€â”€ agent_insights.py         # Agentic AI pipeline
â”œâ”€â”€ Pages/
â”‚   â”œâ”€â”€ Home.py
â”‚   â”œâ”€â”€ Priority_and_Escalation.py
â”‚   â””â”€â”€ Time_Insights.py
â”œâ”€â”€ streamlit_app.py              # Streamlit entry point
â””â”€â”€ requirements.txt              # Python dependencies



â¸»

ğŸš€ Getting Started

1. Clone the Repository

git clone https://github.com/yourusername/ai-jira-analytics.git
cd ai-jira-analytics



â¸»

2. Start Kafka + ClickHouse via Docker

cd Ingest
docker compose up -d

Ensure the following containers are running:

docker ps



â¸»

3. Create the ClickHouse Table

Login to the ClickHouse container:

docker exec -it clickhouse clickhouse-client

Run the schema:

CREATE TABLE jira_issues_raw (
    `Summary` String,
    `Issue key` String,
    `Issue id` Int64,
    `Issue Type` String,
    ...
    `Comment.20` String
) ENGINE = MergeTree ORDER BY `Status`;



â¸»

4. Launch FastAPI (Kafka Producer)

cd Ingest
uvicorn app.main:app --reload

Test the endpoint at:

http://127.0.0.1:8000/docs



â¸»

5. Send Sample Data to Kafka

You can use your CSV loader script or test with:

python app/services/kafka_producer.py



â¸»

6. Start Kafka Consumer to Insert into ClickHouse

python app/services/clickhouse_sink.py

Watch logs for:

Inserted message into ClickHouse



â¸»

7. Launch Streamlit Dashboard

streamlit run streamlit_app.py



â¸»

ğŸ’¡ Features
	â€¢	Real-time ingestion from CSV/API
	â€¢	Sentiment analysis and urgency scoring
	â€¢	ClickHouse-based time filtering and escalation trends
	â€¢	Gemini/Ollama-powered agentic AI bullet summaries
	â€¢	Global filters and rich dashboards using Streamlit + Plotly


â¸»

 License

This project is licensed under the MIT License. Feel free to use, contribute, and fork!

â¸»

Let me know if youâ€™d like me to tailor this README to include badges, CI instructions, or demo images/gifs.
